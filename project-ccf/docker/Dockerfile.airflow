FROM apache/airflow:2.7.3-python3.11

# Switch to root to install system dependencies
USER root

# Install Java for PySpark
RUN apt-get update && \
    apt-get install -y openjdk-11-jre-headless && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Switch back to airflow user
USER airflow

# Install Python dependencies
COPY requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Install additional Airflow providers
RUN pip install --no-cache-dir \
    apache-airflow-providers-postgres \
    apache-airflow-providers-celery \
    apache-airflow-providers-redis

# Set environment variables for Spark
ENV SPARK_HOME=/opt/airflow/.local/lib/python3.11/site-packages/pyspark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYSPARK_PYTHON=python3
ENV PYSPARK_DRIVER_PYTHON=python3

# Create directories
RUN mkdir -p /opt/airflow/logs /opt/airflow/data /opt/airflow/config

# Set working directory
WORKDIR /opt/airflow
